library(glmnet)
library(randomForest)

#' Ensemble Models
#'
#' Fits an ensemble of machine learning models based on user selection.
#' This function allows users to create an ensemble model by selecting multiple machine learning algorithms
#' and specifying their parameters. It supports various types of models including linear and logistic regression,
#' ridge regression, lasso regression, elastic net regression, and random forest. Users can choose which models
#' to include in the ensemble and set their respective parameters.Finally the function fits models specified
#' list to the training data and combines the predictions using a weighted average or voting mechanism
#'
#' @param X The matrix/data frame containing predictors.
#' @param y The response variable.
#'
#' @return Predictions generated by the ensemble of models.
#' @export
#'
#' @examples
#' #Test Case 1: Ensemble of Simple Regression and Ridge Regression
#' # Assuming X and y have been defined
#' X <- matrix(rnorm(100), ncol = 10)  # Sample predictors
#' y <- rnorm(10)  # Sample response variable
#' # Test case parameters
#' test_choices <- c(1, 2)  # Simple Regression and Ridge Regression
#' lambda <- 0.1  # Lambda parameter for Ridge Regression
#' threshold <- 0.5  # Threshold for binary classification
#' # Run ensemble models
#' results1 <- ensemble_models(X, y)
#'
#' @examples
#' #Test Case 2: Ensemble of Lasso Regression and Random Forest
#' # Assuming X and y have been defined
#' X <- matrix(rnorm(100), ncol = 10)  # Sample predictors
#' y <- sample(0:1, 10, replace = TRUE)  # Binary response variable
#' # Test case parameters
#' test_choices <- c(3, 5)  # Lasso Regression and Random Forest
#' lambda <- 0.01  # Lambda parameter for Lasso Regression
#' threshold <- 0.5  # Threshold for binary classification
#' # Run ensemble models
#' results2 <- ensemble_models(X, y)
#'
#' @examples
#' #Test Case 3: Ensemble of Elastic Net Regression and Random Forest
#' # Assuming X and y have been defined
#' X <- matrix(rnorm(100), ncol = 10)  # Sample predictors
#' y <- rnorm(10)  # Continuous response variable
#' # Test case parameters
#' test_choices <- c(4, 5)  # Elastic Net Regression and Random Forest
#' lambda <- 0.01  # Lambda parameter for Elastic Net Regression
#' alpha <- 0.5  # Alpha parameter for Elastic Net Regression
#' threshold <- 0.5  # Threshold for binary classification
#' # Run ensemble models
#' results3 <- ensemble_models(X, y)
ensemble_models <- function(X, y) {

  cat("Available models for ensemble:\n")
  cat("1: Simple Regression (Linear and Logistic Regression)\n")
  cat("2: Ridge Regression\n")
  cat("3: Lasso Regression\n")
  cat("4: Elastic Net Regression\n")
  cat("5: Random Forest\n")
  model_choices <- as.integer(unlist(strsplit(readline(prompt="Enter the model numbers to include, separated by commas (e.g., 1,2): "), ",")))
  model_specs <- list()
  set.seed(123)  # for reproducibility
  if (is.null(colnames(X))) {
    colnames(X) <- paste0("X", seq_len(ncol(X)))
    cat("Column names were not set, default names X1, X2, ..., XN have been assigned.\n")
  }
  mappings <- 0
  if(is.null(X)||is.null(y)){
    cat("please provide test data (x, y)\n")
    return(NULL)
  }

  prs = prescreening(X,y)
  for (choice in model_choices) {
    if (choice == 1) {
      # Parameters for Simple Regression with optional K
      model_specs <- c(model_specs, list(list(name = "Simple Regression", model_function = simple_regression, params = collect_simple_params(prs$predictors, prs$response))))
    } else if (choice == 2) {
      # Parameters for Ridge Regression
      model_specs <- c(model_specs, list(list(name = "Ridge Regression", model_function = ridge_regression_model, params = collect_ridge_params(prs$predictors, prs$response))))
    } else if (choice == 3) {
      # Parameters for Lasso Regression
      model_specs <- c(model_specs, list(list(name = "Lasso Regression", model_function = lasso_regression_model, params = collect_lasso_params(prs$predictors, prs$response))))
    } else if (choice == 4) {
      # Parameters for Elastic Net Regression
      model_specs <- c(model_specs, list(list(name = "Elastic Net Regression", model_function = elasticnet_regression_model, params = collect_elastic_net_params(prs$predictors, prs$response))))
    }
    else if (choice == 5) {
      # Parameters for Random Forest
      model_specs <- c(model_specs, list(list(name = "Random Forest", model_function = random_forest, params = collect_random_forest_params(prs$predictors, prs$response))))
    }
  }

  results <- process_ensemble(prs$predictors, y, model_specs)
  return(results)
}


#' Collect Simple Model Parameters
#'
#' This function collects the parameters required for configuring a simple regression or logistic regression model.
#'
#' @param X The matrix/data frame containing predictors.
#' @param y The response variable.
#' @return A list containing the collected parameters.
#' @export
collect_simple_params <- function(X, y) {
  params <- list()
  return(params)
}

#' Collect Ridge Regression Parameters
#'
#' This function collects the parameters required for configuring a Ridge regression model.
#'
#' @param X The matrix/data frame containing predictors.
#' @param y The response variable.
#' @return A list containing the collected parameters, including lambda.
#' @export
collect_ridge_params <- function(X, y) {
  params <- list()
  params$lambda <- readline(prompt="Enter lambda for Ridge Regression or enter 0 to calculate lambda using CV: ")
  if (as.numeric(params$lambda) == 0) {
    cat("Lambda not specified, performing cross-validation to find lambda.min...\n")
    family = if (length(unique(y))==2) "binomial" else "gaussian"
    X = one_hot_encode(X)
    cv_fit <- cv.glmnet(as.matrix(X), y, alpha = 0 , family = family, type.measure = "deviance", nfolds = 10)
    params$lambda <- cv_fit$lambda.min
    cat("Using lambda.min =", params$lambda, "\n")
  } else {
    params$lambda <- as.numeric(params$lambda)
  }
  return(params)
}

#' Collect Lasso Regression Parameters
#'
#' This function collects the parameters required for configuring a Lasso regression model.
#'
#' @param X The matrix/data frame containing predictors.
#' @param y The response variable.
#' @return A list containing the collected parameters, including lambda.
#' @export
collect_lasso_params <- function(X, y) {
  params <- list()
  params$lambda <- readline(prompt="Enter lambda for Lasso Regression or enter 0 calculate lambda using CV: ")
  if (as.numeric(params$lambda) == 0) {
    cat("Lambda not specified, performing cross-validation to find lambda.min...\n")
    family = if (length(unique(y))==2) "binomial" else "gaussian"
    X = one_hot_encode(X)
    cv_fit <- cv.glmnet(as.matrix(X), y, alpha = 1, family = family, type.measure = "deviance", nfolds = 10)
    params$lambda <- cv_fit$lambda.min
    cat("Using lambda.min =", params$lambda, "\n")
  } else {
    params$lambda <- as.numeric(params$lambda)
  }
  return(params)
}

#' Collect Elastic Net Parameters
#'
#' This function collects the parameters required for configuring an Elastic Net regression model.
#'
#' @param X The matrix/data frame containing predictors.
#' @param y The response variable.
#' @return A list containing the collected parameters, including alpha and lambda.
#' @export
collect_elastic_net_params <- function(X, y) {
  params <- list()
  params$alpha <- as.numeric(readline(prompt="Enter alpha for Elastic Net (0-1): "))
  params$lambda <- as.numeric(readline(prompt="Enter lambda for Elastic Net Regression or enter 0 to calculate using CV: "))
  if (as.numeric(params$lambda) == 0) {
    cat("Lambda not specified, performing cross-validation to find lambda.min...\n")
    X = one_hot_encode(X)
    family = if (length(unique(y))==2) "binomial" else "gaussian"
    cv_fit <- cv.glmnet(as.matrix(X), y, alpha = params$alpha, family = family, type.measure = "deviance", nfolds = 10)
    params$lambda <- cv_fit$lambda.min
    cat("Using lambda.min =", params$lambda, "\n")
  } else {
    params$lambda <- as.numeric(params$lambda)
  }
  return(params)
}

#' Collect Random Forest Parameters
#'
#' This function collects the parameters required for configuring a random forest model.
#'
#' @param X The matrix/data frame containing predictors.
#' @param y The response variable.
#' @return A list containing the collected parameters, including the number of trees (ntrees).
#' @export
collect_random_forest_params <- function(X, y) {
  params <- list()
  params$ntrees <- as.numeric(readline(prompt="Enter the ntree values or enter 0 to use deafult 500: "))
  if (as.numeric(params$ntrees) == 0) {
    cat("ntrees not specified, using deafult value as 500...\n")
    params$ntrees <- 500
  } else {
    params$ntrees <- as.numeric(params$ntrees)
  }
  return(params)
}

#' Process Ensemble Predictions
#'
#' This function fits models specified in the model_specs list to the training data and combines the predictions using a weighted average or voting mechanism.
#'
#' @param X The predictor matrix.
#' @param y The response variable.
#' @param model_specs A list containing specifications for each model to be included in the ensemble.
#' @param threshold The threshold value used for binary classification (default is 0.5).
#'
#' @return Combined predictions from the ensemble.
#'
#' @export
process_ensemble <- function(X, y,model_specs, threshold = 0.5) {
  # Split the data into training and validation sets
  if(length(unique(y)) == 2)
  {
    unique_vals = sort(unique(y))
    mappings <- setNames(unique_vals, c(0, 1))
    binary = 1
  }
  else{
    binary =0
  }

  method <- ifelse(binary==1,"vote","average")
  y = handle_response_variable(y)
  trainIndex <- sample(1:nrow(X), round(0.8 * nrow(X)))
  X_train <- X[trainIndex, ]
  y_train <- y[trainIndex]
  X_val <- X[-trainIndex, ]
  y_val <- y[-trainIndex]

  predictions_list <- list()
  weights <- numeric(length(model_specs))  # Store weights for models

  # Loop over the model specifications
  for (i in seq_along(model_specs)) {
    model_spec <- model_specs[[i]]
    model_function <- model_spec$model_function
    model_params <- model_spec$params
    model_name <- model_spec$name

    # Fit the model using the provided parameters on the training data
    model_fit <- do.call(model_function, c(list(X = X_train, y = y_train), model_params))
    X_val = one_hot_encode(X_val)
    #X_val = as.data.frame(X_val)
    X_val = X_val[ ,model_fit$predictors_names]
    X = one_hot_encode(X)
    X = as.data.frame(X)
    X = X[ ,model_fit$predictors_names]


    # Predict using the fitted model
    if (identical(model_function, simple_regression)) {
      # Check if the model is binary or not
      if (binary ==1 ) {
        # For binary models, return probabilities
        model_predictions <- predict(model_fit$model, newdata = data.frame(X_val), type = "response")
        binary_predictions <- ifelse(model_predictions > threshold, 1, 0)
        #binary_predictions <- mappings[as.character(binary_predictions)]
        accuracy <- mean(binary_predictions == y_val)
        weights[i] <- accuracy
        model_predictions_total <- predict(model_fit$model, newdata = data.frame(X), type = "response")
        binary_predictions_total <- ifelse(model_predictions_total > threshold, 1, 0)
        #binary_predictions_total <- mappings[as.character(binary_predictions_total)]
        predictions_list[[model_name]] <- binary_predictions_total

      } else {
        # For non-binary models, direct prediction
        model_predictions <- predict(model_fit$model, newdata = data.frame(X_val))
        predictions_list[[model_name]] <- model_predictions
        accuracy <- mean(abs(y_val - model_predictions))
        weights[i] <-accuracy
        model_predictions_total <- predict(model_fit$model, newdata = data.frame(X))
        predictions_list[[model_name]] <- model_predictions_total
        #method = "average"
      }
    }
    else if (identical(model_function,random_forest)) {
      # Check if the model is binary or not
      if (binary ==1 ) {
        # For binary models, return probabilities
        model_predictions <- predict(model_fit$model, data.frame(X_val), type = "prob")
        binary_predictions <- ifelse(model_predictions[,2]> threshold, 1, 0)
        #binary_predictions <- mappings[as.character(binary_predictions)]
        accuracy <- mean(binary_predictions == y_val)
        weights[i] <- accuracy
        model_predictions_total <- predict(model_fit$model, data.frame(X), type = "prob")
        binary_predictions_total <- ifelse(model_predictions_total[,2] > threshold, 1, 0)
        #binary_predictions_total <- mappings[as.character(binary_predictions_total)]
        predictions_list[[model_name]] <- binary_predictions_total

      } else {
        # For non-binary models, direct prediction
        model_predictions <- predict(model_fit$model, data.frame(X_val))
        predictions_list[[model_name]] <- model_predictions
        accuracy <- mean(abs(y_val - model_predictions))
        weights[i] <-accuracy
        model_predictions_total <- predict(model_fit$model, data.frame(X))
        predictions_list[[model_name]] <- model_predictions_total
        #method = "average"
      }

    }
    else {
      # Assuming regression_model uses glmnet which requires newx in matrix form and specific lambda
      # Using s = "lambda.min" as a safe default if lambda isn't specifically set

      if (binary==1) {
        model_predictions <- predict(model_fit$model, newx = as.matrix(X_val), s = model_params$lambda, type = "response")
        model_predictions_total <- predict(model_fit$model, newx = as.matrix(X), s = model_params$lambda, type = "response")
        binary_predictions <- ifelse(model_predictions > threshold, 1, 0)
        #binary_predictions <- mappings[as.character(binary_predictions)]
        accuracy <- mean(binary_predictions == y_val)
        weights[i] <- accuracy
        binary_predictions_total <- ifelse(model_predictions_total > threshold, 1, 0)
        #binary_predictions_total <- mappings[as.character(binary_predictions_total)]
        predictions_list[[model_name]] <- binary_predictions_total
        #method = "vote"
      }
      else{
        # Store the predictions with the model name as the key
        model_predictions <- predict(model_fit$model, newx = as.matrix(X_val), s = model_params$lambda)
        model_predictions_total <- predict(model_fit$model, newx = as.matrix(X), s = model_params$lambda)
        predictions_list[[model_name]] <- model_predictions_total
        accuracy <- mean(abs(y_val - model_predictions))
        weights[i] <-accuracy
        #method = "average"
      }
    }
  }

  # Normalize weights
  weights <- weights / sum(weights)

  if (length(model_specs) == 1) {
    warning("Only one model provided. Ensemble prediction requires multiple models for effective aggregation.")
    if(binary == 1)
    {
      combined_predictions <- predictions_list
      #print(combined_predictions)
      combined_predictions <- mappings[as.character(predictions_list)]
      combined_predictions <- unname(combined_predictions)
    }
    else{
      combined_predictions <- predictions_list
    }
  }

  # Combine predictions based on the specified method

  combined_predictions <- NULL
  if (method=="average") {
    combined_predictions <- Reduce(`+`, Map(`*`, predictions_list, weights))
  }
  else if (method == "vote") {
    vote_predictions <- Map(function(pred, weight) {
      pred_logical <- as.logical(pred)
      ifelse(pred_logical, weight, -weight)
    }, predictions_list, weights)

    combined_predictions <- apply(do.call(cbind, vote_predictions), 1, function(x) {
      mean_x <- mean(x, na.rm = TRUE)
      if (mean_x > 0) 1 else 0
    })
    combined_predictions <- mappings[as.character(combined_predictions)]
    combined_predictions <- unname(combined_predictions)
  }

  return(combined_predictions)
}

